<!DOCTYPE html>

<html lang="en">

<head>
  
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><title>朴素贝叶斯与决策树 - fyrhythm&#39;blog</title>
  <meta charset="UTF-8">
  <meta name="description" content="">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5">
  
  

  <link rel="shortcut icon" href="/favicon.ico" type="image/png" />
  <meta name="description" content="贝叶斯分类和决策树模型1.分类问题（1）分类是一种有监督的学习 input： a vector of features  output： a Boolean value(二分类) or integer（多类） （2）有监督学习：需要给每个样本打标签 2.贝叶斯定理 P(A\cup B)&#x3D;P(A)+P(B)-P(A\cap B) P(A\cap B)&#x3D;P(A|B)P(B)&#x3D;P(B|A)P(A) P(">
<meta property="og:type" content="article">
<meta property="og:title" content="朴素贝叶斯与决策树">
<meta property="og:url" content="http://github.com/fyering/fyering.github.io.git/2020/02/26/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%B8%8E%E5%86%B3%E7%AD%96%E6%A0%91/index.html">
<meta property="og:site_name" content="fyrhythm&#39;blog">
<meta property="og:description" content="贝叶斯分类和决策树模型1.分类问题（1）分类是一种有监督的学习 input： a vector of features  output： a Boolean value(二分类) or integer（多类） （2）有监督学习：需要给每个样本打标签 2.贝叶斯定理 P(A\cup B)&#x3D;P(A)+P(B)-P(A\cap B) P(A\cap B)&#x3D;P(A|B)P(B)&#x3D;P(B|A)P(A) P(">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://s2.ax1x.com/2020/02/24/3GkRSO.png">
<meta property="og:image" content="https://s2.ax1x.com/2020/02/24/3Gu4oQ.jpg">
<meta property="og:image" content="https://s2.ax1x.com/2020/02/24/3GJpMd.png">
<meta property="og:image" content="https://s2.ax1x.com/2020/02/26/3U6jv4.md.png">
<meta property="og:image" content="https://s2.ax1x.com/2020/02/26/3UVgMt.md.jpg">
<meta property="og:image" content="https://s2.ax1x.com/2020/02/26/3UZpW9.jpg">
<meta property="og:image" content="https://s2.ax1x.com/2020/02/26/3UeqET.jpg">
<meta property="og:image" content="https://s2.ax1x.com/2020/02/26/3UmoJe.jpg">
<meta property="article:published_time" content="2020-02-26T09:58:55.000Z">
<meta property="article:modified_time" content="2020-02-26T10:00:44.608Z">
<meta property="article:author" content="fyrhythm">
<meta property="article:tag" content="机器学习，数据挖掘">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.ax1x.com/2020/02/24/3GkRSO.png">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/combine/gh/nexmoe/nexmoe.github.io@latest/css/style.css,npm/highlight.js@9.15.8/styles/atom-one-dark.css,gh/nexmoe/nexmoe.github.io@latest/lib/mdui_043tiny/css/mdui.css,gh/nexmoe/nexmoe.github.io@latest/lib/iconfont/iconfont.css" crossorigin>
  
  <!--<link rel="stylesheet" href="/css/style.css?v=1583320111776">-->

  
<meta name="generator" content="Hexo 4.2.0"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>

<body class="mdui-drawer-body-left">
  
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="nexmoe-background">
    <div class="nexmoe-bg" style="background-image: url(https://s2.ax1x.com/2020/02/24/38jY0U.jpg)"></div>
    <div class="mdui-appbar mdui-shadow-0">
      <div class="mdui-toolbar">
        <a mdui-drawer="{target: '#drawer', swipe: true}" title="menu" class="mdui-btn mdui-btn-icon mdui-ripple"><i class="mdui-icon nexmoefont icon-menu"></i></a>
        <div class="mdui-toolbar-spacer"></div>
        <!--<a href="javascript:;" class="mdui-btn mdui-btn-icon"><i class="mdui-icon material-icons">search</i></a>-->
        <a href="/" title="fyrhythm" class="mdui-btn mdui-btn-icon"><img src="http://www.weimeitupian.com/wp-content/uploads/2018/06/2018060808341930.jpg" alt="fyrhythm"></a>
       </div>
    </div>
  </div>
  <div id="nexmoe-header">
      <div class="nexmoe-drawer mdui-drawer" id="drawer">
    <div class="nexmoe-avatar mdui-ripple">
        <a href="/" title="fyrhythm">
            <img src="http://www.weimeitupian.com/wp-content/uploads/2018/06/2018060808341930.jpg" alt="fyrhythm" alt="fyrhythm">
        </a>
    </div>
    <div class="nexmoe-count">
        <div><span>Articles</span>4</div>
        <div><span>Tags</span>1</div>
        <div><span>Categories</span>1</div>
    </div>
    <div class="nexmoe-list mdui-list" mdui-collapse="{accordion: true}">
        
        <a class="nexmoe-list-item mdui-list-item mdui-ripple" href="/" title="回到首页">
            <i class="mdui-list-item-icon nexmoefont icon-home"></i>
            <div class="mdui-list-item-content">
                回到首页
            </div>
        </a>
        
        <a class="nexmoe-list-item mdui-list-item mdui-ripple" href="/archives" title="文章归档">
            <i class="mdui-list-item-icon nexmoefont icon-container"></i>
            <div class="mdui-list-item-content">
                文章归档
            </div>
        </a>
        
    </div>
    <aside id="nexmoe-sidebar">
  
  <div class="nexmoe-widget-wrap">
    <div class="nexmoe-widget nexmoe-search">
        <form id="search_form" action_e="https://cn.bing.com/search?q=site:nexmoe.com" onsubmit="return search();">
            <label><input id="search_value" name="q" type="search" placeholder="Search"></label>
        </form>
    </div>
</div>
  
  <div class="nexmoe-widget-wrap">
    <h3 class="nexmoe-widget-title">Social</h3>
    <div class="nexmoe-widget nexmoe-social">
        <a class="mdui-ripple" href="https://space.bilibili.com/133000099" target="_blank" mdui-tooltip="{content: '哔哩哔哩'}" style="color: rgb(231, 106, 141);background-color: rgba(231, 106, 141, .15);">
            <i class="nexmoefont icon-bilibili"></i>
        </a><a class="mdui-ripple" href="https://github.com/fyering" target="_blank" mdui-tooltip="{content: 'GitHub'}" style="color: rgb(25, 23, 23);background-color: rgba(25, 23, 23, .15);">
            <i class="nexmoefont icon-github"></i>
        </a>
    </div>
</div>
  
  
  <div class="nexmoe-widget-wrap">
    <h3 class="nexmoe-widget-title">Categories</h3>
    <div class="nexmoe-widget">

      <ul class="category-list">

        


        

        

        <li class="category-list-item">
          <a class="category-list-link" href="/categories/数据挖掘与机器学习/">数据挖掘与机器学习</a>
          <span class="category-list-count">3</span>
        </li>

        
      </ul>

    </div>
  </div>


  
  
  <div class="nexmoe-widget-wrap">
    <h3 class="nexmoe-widget-title">Tag Cloud</h3>
    <div id="randomtagcloud" class="nexmoe-widget tagcloud">
      <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%8C%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/" style="font-size: 10px;">机器学习，数据挖掘</a>
    </div>
    
  </div>

  
  
  <div class="nexmoe-widget-wrap">
    <h3 class="nexmoe-widget-title">Archive</h3>
    <div class="nexmoe-widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">March 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">February 2020</a></li></ul>
    </div>
  </div>


  
</aside>
    <div class="nexmoe-copyright">
        &copy; 2020 fyrhythm
        Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
        & <a href="https://nexmoe.com/hexo-theme-nexmoe.html" target="_blank">Nexmoe</a>
    </div>
</div><!-- .nexmoe-drawer -->
  </div>
  <div id="nexmoe-content">
    <div class="nexmoe-primary">
        <div class="nexmoe-post">
  
      <div class="nexmoe-post-cover" style="padding-bottom: 66.66666666666666%;"> 
          <img data-src="https://s2.ax1x.com/2020/02/24/38jY0U.jpg" data-sizes="auto" alt="朴素贝叶斯与决策树" class="lazyload">
          <h1>朴素贝叶斯与决策树</h1>
      </div>
  
  
  <div class="nexmoe-post-meta" style="margin:10px 0!important;">
    <a><i class="nexmoefont icon-calendar-fill"></i>2020年02月26日</a>
    <a><i class="nexmoefont icon-areachart"></i>5.2k 字</a>
    <a><i class="nexmoefont icon-time-circle-fill"></i>大概 22 分钟</a>
</div>

  <div class="nexmoe-post-right">
    
      <div class="nexmoe-fixed">
        <div class="nexmoe-valign">
            <div class="nexmoe-toc">
                <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#贝叶斯分类和决策树模型"><span class="toc-number">1.</span> <span class="toc-text">贝叶斯分类和决策树模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-分类问题"><span class="toc-number">1.1.</span> <span class="toc-text">1.分类问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-贝叶斯定理"><span class="toc-number">1.2.</span> <span class="toc-text">2.贝叶斯定理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-朴素贝叶斯分类器"><span class="toc-number">1.3.</span> <span class="toc-text">3.朴素贝叶斯分类器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-决策树"><span class="toc-number">1.4.</span> <span class="toc-text">4.决策树</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#信号与噪音"><span class="toc-number">1.5.</span> <span class="toc-text">信号与噪音</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#拟合优度"><span class="toc-number">1.6.</span> <span class="toc-text">拟合优度</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#过拟合和欠拟合"><span class="toc-number">1.7.</span> <span class="toc-text">过拟合和欠拟合</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#如何检查过拟合"><span class="toc-number">1.8.</span> <span class="toc-text">如何检查过拟合</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#如何避免过拟合"><span class="toc-number">1.9.</span> <span class="toc-text">如何避免过拟合</span></a></li></ol></li></ol>
            </div>
        </div>
      </div>
    
  </div>

  <article>
    <h1 id="贝叶斯分类和决策树模型"><a href="#贝叶斯分类和决策树模型" class="headerlink" title="贝叶斯分类和决策树模型"></a>贝叶斯分类和决策树模型</h1><h2 id="1-分类问题"><a href="#1-分类问题" class="headerlink" title="1.分类问题"></a>1.分类问题</h2><p>（1）分类是一种有监督的学习</p>
<p>input： a vector of features </p>
<p>output： a Boolean value(二分类) or integer（多类）</p>
<p>（2）有监督学习：需要给每个样本打标签</p>
<h2 id="2-贝叶斯定理"><a href="#2-贝叶斯定理" class="headerlink" title="2.贝叶斯定理"></a>2.贝叶斯定理</h2><script type="math/tex; mode=display">
P(A\cup B)=P(A)+P(B)-P(A\cap B)</script><script type="math/tex; mode=display">
P(A\cap B)=P(A|B)P(B)=P(B|A)P(A)</script><script type="math/tex; mode=display">
P(A|B)=\frac{P(B|A)P(A)}{P(B)}</script><p>例1：</p>
<p>A和B射击，击中靶心的概率如下：</p>
<p>P(A):0.6</p>
<p>P(B):0.5</p>
<p>A和B各自射击一次</p>
<p>已知靶心被命中，那么是A命中的概率是多少？</p>
<script type="math/tex; mode=display">
P(A|C)=\frac{P(C|A)P(A)}{P(C)}=\frac{1*0.6}{0.6*0.5+0.6*0.5+0.4*0.5}</script><p>即A中B不中，A不中B中，A和B都中</p>
<p>从事A命中的概率是75%，但是不代表B的概率为25%，因为A命中的概率和B命中的概率并不是互斥的事件。</p>
<p>例2：</p>
<p>H=头疼</p>
<p>F=得流感</p>
<p>P(H)=1/10,P(F)=1/40;P(H|F)=1/2</p>
<p>得流感的情况下50%的人头疼</p>
<p>问头疼有多少概率得流感？</p>
<p>使用贝叶斯定理：</p>
<script type="math/tex; mode=display">
P(F|H)=\frac{P(H|F)P(F)}{P(H)}=\frac{1/2*1/40}{1/10}=1/8</script><p>出现这种结果得原因是头疼和得流感得先验概率不同</p>
<p><a href="https://imgchr.com/i/3GkRSO" target="_blank" rel="noopener"><img data-sizes="auto" data-src="https://s2.ax1x.com/2020/02/24/3GkRSO.png" alt="3GkRSO.png" class="lazyload"></a></p>
<p>如图所示，红色和蓝色交叉部分在得流感中占了1/2，在头疼中占比例很低。</p>
<h2 id="3-朴素贝叶斯分类器"><a href="#3-朴素贝叶斯分类器" class="headerlink" title="3.朴素贝叶斯分类器"></a>3.朴素贝叶斯分类器</h2><p>独立：</p>
<script type="math/tex; mode=display">
P(A\cap B)=P(A)P(B)</script><p>条件独立：</p>
<script type="math/tex; mode=display">
P(A,B|G)=P(A|G)P(B|G)</script><script type="math/tex; mode=display">
P(A|G,B)=P(A|G)</script><p>当G发生的时候，A和B是相互独立的，但是当G不发生的时候，A和B 不一定相互独立。</p>
<p>例：</p>
<p><img data-sizes="auto" data-src="https://s2.ax1x.com/2020/02/24/3Gu4oQ.jpg" alt="3Gu4oQ.jpg" class="lazyload"></p>
<p>当男性抽烟时得肺癌得概率和抽烟（不考虑性别）时得肺癌得概率是相同时，表明得得肺癌的概率只与抽烟有关（假设抽烟是得肺癌的唯一因素），与性别无关。</p>
<p>  朴素贝叶斯假设自变量之间是具有<strong>条件独立性</strong>的，并且在变量是连续情况下以正态分布为假设。</p>
<p><strong>朴素贝叶斯的思想</strong></p>
<p><img data-sizes="auto" data-src="https://s2.ax1x.com/2020/02/24/3GJpMd.png" alt="3GJpMd.png" class="lazyload"></p>
<p>朴素贝叶斯的思想就是根据先验概率计算某个变量属于某个类别的后验概率。</p>
<p>假如，上表中的信息反映的是某P2P企业判断其客户是否会流失(churn)，而影响到该变量的因素包含年龄、性别、收入、教育水平、消费频次、支持。那根据这样一个信息，我该如何理解朴素贝叶斯的思想呢？再来看一下朴素贝叶斯公式：</p>
<script type="math/tex; mode=display">
P(Y|X)=\frac{P(X|Y)P(Y)}{P(X)}</script><p>从公式中可知，如果要计算X条件下Y发生的概率，<strong>只需要计算出后面等式的三个部分，X事件的概率（P(X)），是X的先验概率、Y属于某类的概率（P(Y)），是Y的先验概率、以及已知Y的某个分类下，事件X的概率（P(X|Y)），是后验概率</strong>。从上表中，是可以计算这三种概率值的。即：</p>
<p><strong>P(x)</strong>指在所有客户集中，某位22岁的本科女性客户，其月收入为7800元，在12次消费中合计支出4000元的概率；</p>
<p><strong>P(Y)</strong>指流失/不流失在所有客户集中的比例；</p>
<p><strong>P(X|Y)</strong>指在已知流失的情况下，一位22岁的本科女性客户，其月收入为7800元，在12次消费中合计支出4000元的概率。</p>
<p><strong>如果要确定某个样本归属于哪一类，则需要计算出归属不同类的概率，再从中挑选出最大的概率。</strong></p>
<p>我们把上面的贝叶斯公式写出这样，也许你能更好的理解：</p>
<script type="math/tex; mode=display">
max(P(Ci|X))=max(\frac{P(X|Ci)P(Ci)}{P(X)})</script><p>而这个公式告诉我们，<strong>需要计算最大的后验概率，只需要计算出分子的最大值即可</strong>，而不同水平的概率P(C)非常容易获得，故难点就在于P(X|C)的概率计算。而问题的解决，正是聪明之处，<strong>即贝叶斯假设变量X间是条件独立的</strong>，故而P(X|C)的概率就可以计算为：</p>
<script type="math/tex; mode=display">
P(X|Ci)=P(X1|Ci)*P(X    2|Ci)*\cdots *P(Xn|Ci)</script><p><strong>对于离散情况：</strong></p>
<p>假设<strong>已知某个客户流失</strong>的情况下，其性别为女，教育水平为本科的概率：</p>
<script type="math/tex; mode=display">
P(gender=F|churn=Y)=2/4=0.5</script><script type="math/tex; mode=display">
P(edu=BK|churn=Y)=2/4=0.5</script><script type="math/tex; mode=display">
P(gender=F,edu=BK|churn=Y)=0.5*0.5=0.25</script><p><strong>上式结果中的分母4为数据集中流失有4条观测，分子2分别是流失的前提下，女性2名，本科2名。</strong></p>
<p>假设<strong>已知某个客户未流失</strong>的情况下，其性别为女，教育水平为本科的概率</p>
<script type="math/tex; mode=display">
P(gender=F|churn=N)=2/3=0.67</script><script type="math/tex; mode=display">
P(edu=BK|churn=N)=2/3=0.67</script><script type="math/tex; mode=display">
P(gender=F,edu=BK|churn=N)=0.67*0.67=0.44</script><p><strong>上式结果中的分母3为数据集中未流失的观测数，分子2分别是未流失的前提下，女性2名，本科2名。</strong></p>
<p><strong>从而P(C|X)公式中的分子结果为：</strong></p>
<script type="math/tex; mode=display">
P(gender=F,edu=BK|churn=Y)*P(churn=Y)=0.14</script><script type="math/tex; mode=display">
P(gender=F,edu=BK|churn=N)*P(churn=N)=01.8</script><p>对于<strong>连续变量的情况就稍微复杂一点</strong>，并非计算频率这么简单，而是<strong>假设该连续变量服从正态分布（即使很多数据并不满足这个条件）</strong>，先来看一下正态分布的密度函数：</p>
<script type="math/tex; mode=display">
f(x)=\frac{1}{\sqrt(2\pi)}e^\frac{(x-u)^2}{2\sigma^2}</script><p>要计算连续变量中某个数值的概率，只需要已知该变量的均值和标准差，再将该数值带入到上面的公式即可。</p>
<h2 id="4-决策树"><a href="#4-决策树" class="headerlink" title="4.决策树"></a>4.决策树</h2><p>决策树模型：决策树由结点和分支构成，内结点是样本属性的一个切分点，叶子结点是样本被决策树划分之后的类或者类的分布（标签）。使用训练样本构建决策树时，通常采用自顶向下的递归方式。</p>
<p>决策树学习包括三个步骤：</p>
<ul>
<li>1.特征选择（即属性选择，又包括切分点选择）</li>
<li>2.决策树生成</li>
<li>3.决策树的修剪</li>
</ul>
<p>对目标样本，不是所有属性一次性就可以决定分类，而是要一步步的。先从一个特征属性的开始判断，如果能直接分类就ok了，如果不行再选一个特征属性再重复上面的判断。如果前几个特征属性可以直接决定类别，别的特征属性的值就会用不上。那么这时就得思考先从哪个特征属性开始？然后是哪个？再后来是哪个？<br>常用的决策树算法有ID3、C4.5、CART、SLIQ、SPRINT等。<br>要了解这些算法，先得了解一些基本概念：</p>
<p>1.信息熵</p>
<p>信息熵可以衡量事务的不确定性，这个事物不确定性越大，信息熵越大。</p>
<p>假如事件A的分类划分为（A1,A2,….,An）吗，每部分发生的概率是（P1,P2,…,Pn）</p>
<script type="math/tex; mode=display">
Ent(P1,P2,...Pn)=-P1\log_2P1-P2\log_2P2-\cdots-Pn\log_2Pn</script><p>那么对于分类来说，数据集类别越多，越不纯，越混乱，则熵越大。反之，熵越小。</p>
<p>2.信息增益</p>
<p>信息增益：在一个条件下，信息不确定性减小的程度。为总的熵减去某个分类标准对应的熵。在决策树分类问题中，就是决策树在进行属性选择划分前和划分后的信息差值。设D是样本集合，属性a有v个可能的值a1,a2,…av,Dv为D中所在属性上取值为av的样本。则用属性a对样本集D进行划分所得的信息增益为：</p>
<script type="math/tex; mode=display">
Gain(D,a)=Ent(D)-\sum^{V}_{v\to1}\frac{Dv}{D}Ent(Dv)</script><p>信息增益越大，意味着使用属性a来进行划分所获得的纯度提升越大。所以决策树在属性选择时，就会选择信息增益大的。信息增益准则对可取值数目较多的属性有所偏好。</p>
<p>3.增益率</p>
<script type="math/tex; mode=display">
Grainratio(D,a)=\frac{Grain(D,a)}{IV(a)}</script><script type="math/tex; mode=display">
IV(a)=-\sum^{V}_{v=1}\frac{Dv}{D}\log_2{\frac{Dv}{D}}</script><p>增益率准则对可取值数目较少的属性有所偏好。</p>
<p>4.基尼指数</p>
<p>数据集D的纯度可用基尼值来度量：</p>
<script type="math/tex; mode=display">
Gini(D)=\sum^{|Y|}_{k=1}\sum^{}_{k1\neq k}=1-\sum^{|Y|}_{k=1}Pk^2</script><p>基尼值GiniD反映了从数据集D中随机出去两个样本，其类别标记不一致的概率。GiniD越小，D的纯度越高。<br>属性a的基尼指数为:</p>
<script type="math/tex; mode=display">
Giniindex(D,a)=\sum^{V}_{v=1}\frac{|Dv|}{|D|}Gini(Dv)</script><p>面我们提到，在构建决策树时，采用的是递归方式。在决策树的基本算法中，递归结束的条件是：程序遍历完所有划分数据集的属性，或者每个分支下的所有实例都具有相同的分类。如果数据集已经处理了所有属性，但是类别标签不是唯一的(即最后一个属性也无法帮助决定样本归属于哪个类)，此时我们需要决定如何定义该叶子结点，在这种情况下，我们通常会采用多数表决的方法决定该叶子结点的分类。</p>
<p>ID3算法（鼻祖算法）<br>ID3算法是一种贪心算法，用来构造决策树。以信息熵和信息增益为衡量标准。在划分数据集时，每次划分选取信息增益最高的属性进行划分，重复这个过程，直到生成一个能完美分类训练样例的决策树。</p>
<p>步骤：</p>
<p>输入：样本集合S,属性集合A</p>
<p>输出：ID3决策树</p>
<p>遇到递归终止的条件，返回类别；否则执行（2）</p>
<p> 计算出信息增益最大的属性a,把a作为作为根节点。生成属性集合A1=A-a,执行（3）</p>
<p>对属性a的的每个可能的取值，执行：</p>
<ol>
<li><p>将所有属性a的值是v的样本作为S的一个子集Sv;</p>
</li>
<li><p>以样本集合Sv和属性集合A1为输入，递归执行ID3算法。<br>优点：实现比较简单，产生的规则如果用图表示出来的话，清晰易懂，分类效果好。</p>
</li>
</ol>
<p>缺点：只能处理<strong>标称型数据</strong>，容易形成过拟合，在选择最佳划分属性时容易选择那些属性值多的一些属性。</p>
<p>C4.5<br>C4.5算法继承了ID3算法的优点，并在以下几个方面对ID3算法进行了改进：</p>
<p>用信息增益率来选择属性，克服了用信息增益选择属性时偏向选择取值多的属性的不足；</p>
<p>在树构造过程中进行了剪枝，避免了过拟合；</p>
<p>能够完成对连续属性的离散化处理；</p>
<p>能够对不完整数据进行处理。<br>优点：产生的分类规则易于理解，准确率较高。</p>
<p>缺点：在树的构造过程中，需要对数据集进行多次的顺序扫描和排序，耗时。</p>
<p>增益率准则对可取值数目较少的属性有所偏好。因此，C4.5算法并不是直接选择增益率最大的候选划分属性，而是使用了一个启发式：先从候选划分属性中找出信息增益高于平均水平的，再从中选择增益率最高的。</p>
<p>CART</p>
<p>CART决策树使用“基尼指数”来选择划分属性。</p>
<p>基尼指数最小的为最优划分属性。</p>
<p>剪枝是对付过拟合的主要手段。分为预剪枝和后剪枝。 主要当前结点的划分能不能使决策树的性能提升来判断要不要剪枝。</p>
<p>过拟合现象：模型从训练数据到新数据的泛化能力并不好</p>
<p>剪枝：</p>
<p>剪枝并非真的剪，而是合并（使用少数服从多数的方式）。原数据集在训练集上构造的决策树可能很深（与样本的属性相关），但是在使用校验集对决策树进行剪枝，使得决策树变得简单，因此能够校验集可以解决决策树过学习的问题，从而在对测试集进行分类时表现良好。</p>
<p><a href="https://imgchr.com/i/3U6jv4" target="_blank" rel="noopener"><img data-sizes="auto" data-src="https://s2.ax1x.com/2020/02/26/3U6jv4.md.png" alt="3U6jv4.md.png" class="lazyload"></a></p>
<p>有些属性不适合做分类，比如根据人的生日来判断人的性别是不合理的，因为根据生日构造决策树时，ID3算法倾向于属性划分后的子集的熵为0，此时就会将每个样本作为一条规则，常常会将属性分的很零碎。这种会很容易导致过学习。</p>
<p>增益率解决了上述问题，根据增益率的公式可知，属性被划分的越细，增益率的值越大。相当于是属性的一个惩罚量，防止属性被划分的过多。</p>
<p>连续型属性：</p>
<p>例如温度，需要取一个阈值（threshold），将连续性属性变为离散型属性。</p>
<h2 id="信号与噪音"><a href="#信号与噪音" class="headerlink" title="信号与噪音"></a><strong>信号与噪音</strong></h2><p>您可能听说过Nate Silver著名的《信号与噪音》一书。在预测建模中，您可以将“信号”视为希望从数据中学习到的真正底层模式。另一方面，“噪音”指的是数据集中无关的信息或随机性。例如，假设您正在建模儿童身高与年龄的关系。如果您对大部分人口进行抽样，您会发现一个非常明确的关系：</p>
<p><a href="https://imgchr.com/i/3UVgMt" target="_blank" rel="noopener"><img data-sizes="auto" data-src="https://s2.ax1x.com/2020/02/26/3UVgMt.md.jpg" alt="3UVgMt.md.jpg" class="lazyload"></a></p>
<p><strong>这就是是信号</strong>。然而，如果你只能对当地的一所学校进行抽样调查，这种关系可能会更加复杂。 它会受到异常值(比如，爸爸是NBA球员的孩子)和随机性(例如在不同年龄段进入青春期的孩子)的影响。</p>
<p><strong>“噪音干扰了信号”</strong></p>
<p>这成为机器学习的用武之地，一个运行良好的机器学习算法能将信号从噪声中分离出来。</p>
<p>如果算法过于复杂或灵活(例如，它有太多的输入特性或它没有适当的正则化)，它最终可能“记住噪音”而不是找到信号。</p>
<p>这个过拟合模型将基于这些噪声进行预测。它将在训练数据上表现得异常出色……但在新的、未见过的数据上表现得非常糟糕。</p>
<h2 id="拟合优度"><a href="#拟合优度" class="headerlink" title="拟合优度"></a><strong>拟合优度</strong></h2><p>在统计学中，拟合优度是指模型的预测值与观测值(真实)的匹配程度。一个学习了噪声而不是信号的模型被认为是“过拟合”的，因为它适合训练数据集，但与新数据集的拟合度较差。</p>
<p><a href="https://imgchr.com/i/3UZpW9" target="_blank" rel="noopener"><img data-sizes="auto" data-src="https://s2.ax1x.com/2020/02/26/3UZpW9.jpg" alt="3UZpW9.jpg" class="lazyload"></a></p>
<h2 id="过拟合和欠拟合"><a href="#过拟合和欠拟合" class="headerlink" title="过拟合和欠拟合"></a><strong>过拟合和欠拟合</strong></h2><p>通过观察相反的问题，我们可以更好地理解欠拟合。 当一个模型过于简单时，因为它的特性太少，或者过于正则化，就会出现欠拟合现象，这使得它在学习数据集时不够灵活。简单的模型在预测结果上往往有较小的方差和较大的偏差（见：偏差-方差权衡）。另一方面，复杂的模型往往在预测中有更大的方差。</p>
<p><strong>偏差和方差都是机器学习中预测误差的两种形式。</strong></p>
<p>通常，我们可以减少由偏差引起的误差，但同时可能会增加由方差带来的误差，反之亦然。太简单（高偏差）与过于复杂（高方差）之间的权衡是统计和机器学习中的关键概念，也是影响所有监督学习算法的关键概念。</p>
<h2 id="如何检查过拟合"><a href="#如何检查过拟合" class="headerlink" title="如何检查过拟合"></a><strong>如何检查过拟合</strong></h2><p>过拟合以及机器学习的一个关键挑战是，在我们实际测试之前，我们无法知道模型对新数据的执行情况。为了解决这个问题，我们可以将初始数据集拆分为单独的<em>训练</em>和<em>测试</em>子集。</p>
<p>这种方法可以估计出我们的模型在新数据上的表现。</p>
<p><strong>如果我们的模型在训练集上比在测试集中表现得好得多，那么我们很可能会过拟合。</strong></p>
<p>例如，如果我们的模型在训练集上有99％的准确率，但在测试集上只有55％的准确率，那将是一个很危险的信号。</p>
<p><strong>另一个建议是从一个非常简单的模型开始，以此作为基准。</strong></p>
<p>然后，当您尝试更复杂的算法时，您将有一个参考基准来查看额外的复杂性是否值得。这是奥卡姆剃刀试验。如果两个模型具有类似的性能，那么通常应该选择比较简单的一个。</p>
<h2 id="如何避免过拟合"><a href="#如何避免过拟合" class="headerlink" title="如何避免过拟合"></a><strong>如何避免过拟合</strong></h2><p>检查过拟合是有用的，但它不能解决问题。幸运的是，有几个方法您可以尝试。以下是一些最常用的过拟合解决方案：</p>
<p><strong>1.交叉验证</strong></p>
<p>交叉验证是预防过拟合的一个强有力措施。</p>
<p>将您的初始训练数据拆分成多个数据集（类似于迷你火车），使用这些拆分子集来调整模型，这是一个聪明的想法。在标准的K-fold交叉验证中，我们将数据划分为K个子集，称为“折叠(folds)”。然后我们迭代地在K-1个折叠上训练算法，同时使用剩余的折叠作为测试集。</p>
<p>交叉验证允许您仅使用原始训练集来调整超参数。这使您可以将测试集保存为真正“未见过”的数据集，以便选择最终模型。</p>
<p><img data-sizes="auto" data-src="https://s2.ax1x.com/2020/02/26/3UeqET.jpg" alt="3UeqET.jpg" class="lazyload"></p>
<p><strong>2.使用更多数据</strong></p>
<p>它不会每次都有效，但是使用更多数据进行训练可以帮助算法更好地检测到信号。</p>
<p>在早期的儿童身高与年龄建模的例子中，很明显如何抽样更多的学校将有助于您的模型。当然，情况并非总是如此。如果我们只是添加更多的噪声数据，这种技术将无济于事。这就是为什么您应该始终确保您的数据是干净和相关的。</p>
<p><strong>3.删除无用特征</strong></p>
<p>有些算法有内置的特征选择。 您可以通过删除不相关的输入特性来手动改进它们的通用性。</p>
<p>一种有趣的方法是通过描述每个特性是如何融入模型的。如果很难证明一些特性的存在合理性，说明这些特征是没必要的。</p>
<p><strong>4.及时中止</strong></p>
<p>当您迭代训练学习算法时，您可以度量模型的每次迭代的执行情况。</p>
<p>当迭代至一定次数之前，新的迭代会不断改进模型。然而，在那之后，模型的泛化能力会随着训练数据开始过拟合而减弱。</p>
<p><img data-sizes="auto" data-src="https://s2.ax1x.com/2020/02/26/3UmoJe.jpg" alt="3UmoJe.jpg" class="lazyload"></p>
<p>现在，这种方法主要用于深度学习，而其他的方法(如正则化)更适合于经典的机器学习。</p>
<p><strong>5.正则化</strong></p>
<p>正则化是指人为地迫使模型变得更简单的一系列技术。这个方法将取决于你使用的模型类型。例如，您可以修剪决策树，在神经网络上使用dropout，或者在回归中向代价函数添加一个惩罚参数。</p>
<p>通常，正则化方法也是一个超参数，这意味着它可以通过交叉验证进行调优。</p>
<p><strong>6.集成学习</strong></p>
<p>集成(Ensembling)是一种机器学习方法，用于将多个不同模型的预测组合在一起。</p>
<p>集成有几种不同的方法，但最常见的两种是:</p>
<p><strong>Bagging</strong>：降低复杂模型过拟合的可能性。</p>
<ul>
<li>它同时训练大量“强大”的模型。</li>
<li>一个“强大”的模型是一个相对不受约束的模型。</li>
<li>然后将所有“强大”的模型结合在一起，以“平滑”他们的预测。</li>
</ul>
<p><strong>Boosting</strong>：改进简单模型的预测能力。</p>
<ul>
<li>它训练大量“弱”的模型。</li>
<li>一个“弱”模型是一个受约束的模型(例如，你可以限制每个决策树的最大深度)。</li>
<li>每个模型都专注于从之前的错误中学习。</li>
<li>然后把所有的弱学习者组合成一个强大的学习者。</li>
</ul>
<p>虽然Bagging和Boosting都是集成方法，但它们从相反的方向解决问题。Bagging使用复杂的基础模型，试图“平滑”他们的预测，而Boosting使用简单的基础模型，并试图“提高”他们的总复杂度。</p>
<p>原文见：</p>
<p><a href="https://zhuanlan.zhihu.com/p/40516287" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/40516287</a></p>

  </article>

  <div class="nexmoe-post-meta">
    
        <a class="nexmoefont icon-appstore-fill -link" href="/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">数据挖掘与机器学习</a>
    
    
        <a class="nexmoefont icon-tag-fill -link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%8C%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/" rel="tag">机器学习，数据挖掘</a>
    
</div>

  <div class="nexmoe-post-footer">
    
      
  <div class="nexmoe-post-copyright">
    <strong>Author：</strong>fyrhythm<br>
    <strong>Link：</strong><a href="http://github.com/fyering/fyering.github.io.git/2020/02/26/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%B8%8E%E5%86%B3%E7%AD%96%E6%A0%91/" title="http:&#x2F;&#x2F;github.com&#x2F;fyering&#x2F;fyering.github.io.git&#x2F;2020&#x2F;02&#x2F;26&#x2F;%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%B8%8E%E5%86%B3%E7%AD%96%E6%A0%91&#x2F;" target="_blank" rel="noopener">http:&#x2F;&#x2F;github.com&#x2F;fyering&#x2F;fyering.github.io.git&#x2F;2020&#x2F;02&#x2F;26&#x2F;%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%B8%8E%E5%86%B3%E7%AD%96%E6%A0%91&#x2F;</a><br>
    
      <strong>版权声明：</strong>本文采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/cn/deed.zh" target="_blank">CC BY-NC-SA 3.0 CN</a> 协议进行许可
    
  </div>


    
    <section class="nexmoe-comment">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.5.0/dist/gitalk.min.css">
<div id="gitalk"></div>
<script src="https://cdn.jsdelivr.net/npm/gitalk@1.5.0/dist/gitalk.min.js"></script>
<script type="text/javascript">
    var gitalk = new Gitalk({
        clientID: '80b2453b6d5f37ad6225',
        clientSecret: '43e99fa852795c9a7b3eb924b2558c64b84bbdeb',
        id: window.location.pathname,
        repo: 'nexmoe.github.io',
        owner: 'nexmoe',
        admin: 'nexmoe'
    })
    gitalk.render('gitalk')
</script>
</section>
  </div>
</div>
    </div>
  </div>
  <script src="https://cdn.jsdelivr.net/combine/npm/lazysizes@5.1.0/lazysizes.min.js,gh/highlightjs/cdn-release@9.15.8/build/highlight.min.js,npm/mdui@0.4.3/dist/js/mdui.min.js,gh/nexmoe/nexmoe.github.io@latest/js/app.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<!--<script src="/js/app.js?v=1583320111785"></script>-->


    <script src="https://cdn.jsdelivr.net/gh/xtaodada/xtaodada.github.io@0.0.2/copy.js"></script>


 
<script> 
!function(e,t,a){function n(){c(".heart{width: 10px;height: 10px;position: fixed;background: #f00;transform: rotate(45deg);-webkit-transform: rotate(45deg);-moz-transform: rotate(45deg);}.heart:after,.heart:before{content: '';width: inherit;height: inherit;background: inherit;border-radius: 50%;-webkit-border-radius: 50%;-moz-border-radius: 50%;position: fixed;}.heart:after{top: -5px;}.heart:before{left: -5px;}"),o(),r()}function r(){for(var e=0;e<d.length;e++)d[e].alpha<=0?(t.body.removeChild(d[e].el),d.splice(e,1)):(d[e].y--,d[e].scale+=.004,d[e].alpha-=.013,d[e].el.style.cssText="left:"+d[e].x+"px;top:"+d[e].y+"px;opacity:"+d[e].alpha+";transform:scale("+d[e].scale+","+d[e].scale+") rotate(45deg);background:"+d[e].color+";z-index:99999");requestAnimationFrame(r)}function o(){var t="function"==typeof e.onclick&&e.onclick;e.onclick=function(e){t&&t(),i(e)}}function i(e){var a=t.createElement("div");a.className="heart",d.push({el:a,x:e.clientX-5,y:e.clientY-5,scale:1,alpha:1,color:s()}),t.body.appendChild(a)}function c(e){var a=t.createElement("style");a.type="text/css";try{a.appendChild(t.createTextNode(e))}catch(t){a.styleSheet.cssText=e}t.getElementsByTagName("head")[0].appendChild(a)}function s(){return"rgb("+~~(255*Math.random())+","+~~(255*Math.random())+","+~~(255*Math.random())+")"}var d=[];e.requestAnimationFrame=function(){return e.requestAnimationFrame||e.webkitRequestAnimationFrame||e.mozRequestAnimationFrame||e.oRequestAnimationFrame||e.msRequestAnimationFrame||function(e){setTimeout(e,1e3/60)}}(),n()}(window,document);
</script>

  





<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>

</html>
